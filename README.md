# Small Language Model Reasoning using Multi-Agent Graph Distillation 
Reimplemention of the research paper [MAGDI: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models](https://arxiv.org/pdf/2402.01620v2) using structured knowledge distillation to improve reasoning of small language models from integrating multi-agent interaction graph with LLMs.  

1. Prepare MAGs Training Data 
2. Get Node Embeddings 
3. Train the Base Student Model using MAGDi 
4. Evaluate the MAGDi-Augmented Model

## Multi-Agent System
![image](https://github.com/user-attachments/assets/c5af52e2-7568-4c51-a9fb-920c698fc083)
![image](https://github.com/user-attachments/assets/032ab31b-13c5-484c-9124-a6d476e0f71a)
